{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from jcm import ClusterModel, run_gibbs\n",
    "from jcm.utils import estimate_cluster\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import multivariate_normal, dirichlet, linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "PLOTS_DIR = \"./plots/\"\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions and basic test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(N=100, V=50, doc_length=20, eta=0.1, z_true=None):\n",
    "    if z_true is None or len(z_true) != N:\n",
    "        raise ValueError(\"z_true must be provided and match N\")\n",
    "    \n",
    "    K = 2  # fixed to 2 clusters\n",
    "    word_probs = dirichlet.rvs([eta] * V, size=K)  # two topic distributions\n",
    "\n",
    "    W = np.zeros((N, V), dtype=int)\n",
    "    for i in range(N):\n",
    "        k = z_true[i]\n",
    "        W[i] = np.random.multinomial(doc_length, word_probs[k])\n",
    "    \n",
    "    return W, word_probs\n",
    "\n",
    "def generate_embeddings(N=100, separation=2.0, sigma=1.0, z_true=None):\n",
    "    if z_true is None or len(z_true) != N:\n",
    "        raise ValueError(\"z_true must be provided and match N\")\n",
    "    \n",
    "    # Two means along the y = x line\n",
    "    direction = np.array([1.0, 1.0]) / np.sqrt(2)\n",
    "    mu1 = -0.5 * separation * direction\n",
    "    mu2 =  0.5 * separation * direction\n",
    "    means = [mu1, mu2]\n",
    "    \n",
    "    cov = sigma**2 * np.eye(2)\n",
    "\n",
    "    Y = np.zeros((N, 2))\n",
    "    for i in range(N):\n",
    "        k = z_true[i]\n",
    "        Y[i] = multivariate_normal.rvs(mean=means[k], cov=cov)\n",
    "    \n",
    "    return Y, means\n",
    "\n",
    "\n",
    "def plot_2d_embeddings(Y, z, title=\"Embeddings (2D)\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    scatter = ax.scatter(Y[:, 0], Y[:, 1], c=z, cmap=\"tab10\", edgecolors=\"k\", alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "    return scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randint(0, 2, size=100)  # Random cluster assignments\n",
    "W, word_probs = generate_words(N=100, V=10, doc_length=10, eta=0.1, z_true=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate setup\n",
    "N = 200\n",
    "z_true = np.array([0]*100 + [1]*100)\n",
    "sep = 1.5\n",
    "d = 2\n",
    "W, word_probs = generate_words(N=N, V=50, doc_length=10, eta=0.1, z_true=z_true)\n",
    "X, means = generate_embeddings(N=N, separation=sep, sigma=0.5, z_true=z_true)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "plot_2d_embeddings(X, z_true, ax=ax, title=f\"Embeddings with true labels, separation={sep}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GMM to set initial cluster assignments based on embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "gmm = GaussianMixture(n_components=K, covariance_type='full', random_state=42)\n",
    "gmm.fit(X)\n",
    "labels = gmm.predict(X)\n",
    "fig, ax = plt.subplots()\n",
    "plot_embeddings(X, labels, ax, title=\"GMM Labels\")\n",
    "print(f\"ARI: {adjusted_rand_score(z_true, labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.zeros(d)\n",
    "for k in range(K):\n",
    "    cluster_k = X[labels == k]\n",
    "    if len(cluster_k) == 0:\n",
    "        raise ValueError(f\"Cluster {k} has no points assigned.\")\n",
    "    v += np.var(cluster_k, axis=0) / K\n",
    "\n",
    "S_0 = np.diag(v)  # Shared across all clusters\n",
    "m_0 = np.zeros(d)  # Mean vector for the prior\n",
    "kappa_0 = 0.05\n",
    "\n",
    "# Initialise model using GMM labels as predictions\n",
    "model = ClusterModel(X, W, K, labels, m_0, S_0, kappa_0=kappa_0)\n",
    "\n",
    "M = 500\n",
    "burnin = 200\n",
    "\n",
    "samples, likelihoods = run_gibbs(model, M, burnin, track_likelihood=True, show_progress=True)\n",
    "z_pred = estimate_cluster(samples, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_embeddings(X, z_pred, ax, title=\"M4R model Labels\")\n",
    "print(f\"ARI: {adjusted_rand_score(z_true, z_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "N = 200\n",
    "K = 2\n",
    "V = 50\n",
    "doc_length = 10\n",
    "eta = 0.5\n",
    "sigma = 0.5\n",
    "z_true = np.array([0]*100 + [1]*100)\n",
    "\n",
    "# Separation values (decreasing)\n",
    "separations = [3.0, 2.0, 1.5, 1.0, 0.5]\n",
    "\n",
    "# Set up the plot grid\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate data and run model for each separation\n",
    "for idx, sep in enumerate(separations):\n",
    "    # Generate data\n",
    "    W, word_probs = generate_words(N=N, V=V, doc_length=doc_length, eta=eta, z_true=z_true)\n",
    "    X, means = generate_embeddings(N=N, separation=sep, sigma=sigma, z_true=z_true)\n",
    "    \n",
    "    # Initialize GMM for initial cluster assignments\n",
    "    gmm = GaussianMixture(n_components=K, covariance_type='full', random_state=42)\n",
    "    gmm.fit(X)\n",
    "    initial_labels = gmm.predict(X)\n",
    "    \n",
    "    m_0 = np.zeros(X.shape[1])  # Mean vector for the prior\n",
    "    v = np.zeros(d)\n",
    "    for k in range(K):\n",
    "        cluster_k = X[labels == k]\n",
    "        if len(cluster_k) == 0:\n",
    "            raise ValueError(f\"Cluster {k} has no points assigned.\")\n",
    "        v += np.var(cluster_k, axis=0) / K\n",
    "\n",
    "    S_0 = np.diag(v)  # Shared across all clusters\n",
    "\n",
    "    # Initialise model using GMM labels as predictions\n",
    "    model = ClusterModel(X, W, K, labels, m_0, S_0, kappa_0=kappa_0)\n",
    "    samples, _ = run_gibbs(model, M=500, burnin=200)\n",
    "    z_pred = estimate_cluster(samples, K)\n",
    "\n",
    "    # Compute ARI\n",
    "    ari = adjusted_rand_score(z_true, z_pred)\n",
    "    \n",
    "    # Plot results\n",
    "    ax = axes[idx]\n",
    "    plot_2d_embeddings(X, model.z, title=f\"Separation = {sep}, ARI = {ari:.3f}\", ax=ax)\n",
    "\n",
    "# Remove empty subplot (if any)\n",
    "if len(separations) < len(axes):\n",
    "    for i in range(len(separations), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR + 'separation_grid.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting separation against ARI, with lines for GMM model and M4R model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "N = 200\n",
    "K = 2\n",
    "V = 50\n",
    "doc_length = 20\n",
    "sigma = 0.5\n",
    "\n",
    "# Separation values (decreasing)\n",
    "separations = np.arange(0, 3, 0.1)\n",
    "M = 20\n",
    "eta_values = [0.5, 1.0, 5.0, 10.0]\n",
    "\n",
    "ari_m4r = np.zeros((len(eta_values), len(separations), M))\n",
    "ari_gmm = np.zeros((len(eta_values), len(separations), M))\n",
    "\n",
    "# Generate data and run model for each separation\n",
    "for i, eta in enumerate(eta_values):\n",
    "    # Fix cluster labels and word distributions (weakly informative)\n",
    "    z_true = np.array([0]*100 + [1]*100)\n",
    "    W, word_probs = generate_words(N=N, V=V, doc_length=doc_length, eta=eta, z_true=z_true)\n",
    "    for j, sep in enumerate(separations):\n",
    "        for k in range(M): \n",
    "            # Generate data\n",
    "            X, means = generate_embeddings(N=N, separation=sep, sigma=sigma, z_true=z_true)\n",
    "            \n",
    "            # Initialize GMM for initial cluster assignments\n",
    "            gmm = GaussianMixture(n_components=K, covariance_type='full', random_state=42)\n",
    "            gmm.fit(X)\n",
    "            initial_labels = gmm.predict(X)\n",
    "\n",
    "            ari_gmm[i, j, k] = adjusted_rand_score(z_true, initial_labels)\n",
    "            \n",
    "            # Set NIW priors\n",
    "            m_0 = np.zeros(X.shape[1])  # Mean vector for the prior\n",
    "            \n",
    "            v = np.zeros(d)\n",
    "            for c in range(K):\n",
    "                cluster_k = X[initial_labels == c]\n",
    "                if len(cluster_k) == 0:\n",
    "                    raise ValueError(f\"Cluster {c} has no points assigned.\")\n",
    "                v += np.var(cluster_k, axis=0) / K\n",
    "\n",
    "            S_0 = np.diag(v)  # Shared across all clusters\n",
    "            m_0 = np.zeros(d)  # Mean vector for the prior\n",
    "            kappa_0 = 0.05\n",
    "\n",
    "            # Initialise model using GMM labels as predictions\n",
    "            model = ClusterModel(X, W, K, initial_labels, m_0, S_0, kappa_0=kappa_0)\n",
    "\n",
    "            n_iter = 10\n",
    "            burnin = 2\n",
    "\n",
    "            samples, _ = run_gibbs(model, n_iter, burnin)\n",
    "            z_pred = estimate_cluster(samples, K)\n",
    "\n",
    "            # Compute ARI\n",
    "            ari_m4r[i, j, k] = adjusted_rand_score(z_true, z_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ARI results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot a separate M4R curve for each eta value\n",
    "for i, eta_val in enumerate(eta_values):\n",
    "    mean_ari = np.mean(ari_m4r[i], axis=1)\n",
    "    std_ari = np.std(ari_m4r[i], axis=1)\n",
    "    ax.plot(separations, mean_ari, label=f'M4R $\\eta = $ {eta_val:.1f}')\n",
    "    ax.fill_between(separations, mean_ari - std_ari, mean_ari + std_ari, alpha=0.2)\n",
    "\n",
    "# Plot GMM (shared across all eta values)\n",
    "mean_ari_gmm = np.mean(ari_gmm, axis=(0, 2))\n",
    "std_ari_gmm = np.std(ari_gmm, axis=(0, 2))\n",
    "ax.plot(separations, mean_ari_gmm, label='GMM', color='black')\n",
    "ax.fill_between(separations, mean_ari_gmm - std_ari_gmm, mean_ari_gmm + std_ari_gmm, color='black', alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Separation')\n",
    "ax.set_ylabel('ARI')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR + 'ari_vs_separation_eta.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate likelihood weighting by $\\alpha$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "N = 200\n",
    "K = 2\n",
    "V = 50\n",
    "doc_length = 20\n",
    "sigma = 0.5\n",
    "\n",
    "# Separation values (finer grid for 0 to 0.5)\n",
    "separations = np.concat([np.arange(0, 0.5, 0.025), np.arange(0.5, 3, 0.25)])\n",
    "M = 20\n",
    "alpha_values = [0.05, 0.1, 0.15, 0.2, 0.5, 1.0]\n",
    "\n",
    "ari_m4r = np.zeros((len(alpha_values), len(separations), M))\n",
    "ari_gmm = np.zeros((len(alpha_values), len(separations), M))\n",
    "\n",
    "# Fix cluster labels and word distributions (weakly informative)\n",
    "z_true = np.array([0]*100 + [1]*100)\n",
    "eta=0.5\n",
    "W, word_probs = generate_words(N=N, V=V, doc_length=doc_length, eta=eta, z_true=z_true)\n",
    "\n",
    "# Generate data and run model for each weighting\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    for j, sep in enumerate(separations):\n",
    "        for k in range(M): \n",
    "            # Generate data\n",
    "            X, means = generate_embeddings(N=N, separation=sep, sigma=sigma, z_true=z_true)\n",
    "\n",
    "            # Initialize GMM for initial cluster assignments\n",
    "            gmm = GaussianMixture(n_components=K, covariance_type='full', random_state=42)\n",
    "            gmm.fit(X)\n",
    "            initial_labels = gmm.predict(X)\n",
    "\n",
    "            ari_gmm[i, j, k] = adjusted_rand_score(z_true, initial_labels)\n",
    "            \n",
    "            # Set NIW priors\n",
    "            m_0 = np.zeros(X.shape[1])  # Mean vector for the prior\n",
    "            \n",
    "            v = np.zeros(d)\n",
    "            for c in range(K):\n",
    "                cluster_k = X[initial_labels == c]\n",
    "                if len(cluster_k) == 0:\n",
    "                    raise ValueError(f\"Cluster {c} has no points assigned.\")\n",
    "                v += np.var(cluster_k, axis=0) / K\n",
    "\n",
    "            S_0 = np.diag(v)  # Shared across all clusters\n",
    "            m_0 = np.zeros(d)  # Mean vector for the prior\n",
    "            kappa_0 = 0.05\n",
    "\n",
    "            # Initialise model using GMM labels as predictions\n",
    "            model = ClusterModel(X, W, K, initial_labels, m_0, S_0, kappa_0=kappa_0, weight_W = alpha)\n",
    "\n",
    "            n_iter = 10\n",
    "            burnin = 2\n",
    "\n",
    "            samples, _ = run_gibbs(model, n_iter, burnin)\n",
    "            z_pred = estimate_cluster(samples, K)\n",
    "\n",
    "            # Compute ARI\n",
    "            ari_m4r[i, j, k] = adjusted_rand_score(z_true, z_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ARI results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "\n",
    "# Plot GMM (shared across all eta values)\n",
    "mean_ari_gmm = np.mean(ari_gmm[0], axis=1)\n",
    "std_ari_gmm = np.std(ari_gmm[0], axis=1)\n",
    "ax.plot(separations, mean_ari_gmm, label=r'GMM ($\\alpha_\\mathbf{{W}} = 0$)', color='black')\n",
    "lower_bound = np.clip(mean_ari_gmm - std_ari_gmm, 0, 1)\n",
    "upper_bound = np.clip(mean_ari_gmm + std_ari_gmm, 0, 1)\n",
    "ax.fill_between(separations, lower_bound, upper_bound, color='black', alpha=0.2)\n",
    "\n",
    "# Plot a separate M4R curve for each eta value\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    mean_ari = np.mean(ari_m4r[i], axis=1)\n",
    "    std_ari = np.std(ari_m4r[i], axis=1)\n",
    "    ax.plot(separations, mean_ari, label=rf'$\\alpha_\\mathbf{{W}} = $ {alpha:.2f}')\n",
    "    lower_bound = np.clip(mean_ari - std_ari, 0, 1)\n",
    "    upper_bound = np.clip(mean_ari + std_ari, 0, 1)\n",
    "    ax.fill_between(separations, lower_bound, upper_bound, alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Separation')\n",
    "ax.set_ylabel('Adjusted Rand Index (ARI)')\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR + 'ari_vs_separation_alpha.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finer grid between 0.5 0\n",
    "more separations between 0 and 0.1\n",
    "\n",
    "plot on Cora dataset, plotting ari against alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Dirichlet prior hyperparameter.\n",
    "\n",
    "1. $\\eta$ - (controls $\\phi$) Dirichlet prior for word distributions spiky vs uniform word distribution within clusters\n",
    "2. $\\gamma$ - (controls $\\psi$) Dirichlet prior for cluster assigments, smaller gamma favours uneven cluster assignments, larger gamma encourages more balanced cluster sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the document length, show how when we increase it becomes dominat over the likelihood, could compare the likelihood components $l_w$ vs $l_e$., weighted mix over $l_w$ and $l_e$.\n",
    "\n",
    "Weight the posterior z prob by document lenght/embedding dimension i.e. rescale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment settings\n",
    "N, d, K, V = 200, 2, 2, 100\n",
    "cluster_sep, doc_length = 1, 100\n",
    "\n",
    "M = 50 # Number of simulations per eta\n",
    "eta_values = np.arange(0.1, 10.5, 0.5)\n",
    "ari_values = np.zeros((len(eta_values), M))\n",
    "js_divergence = np.zeros((len(eta_values), M))\n",
    "# Sample cluster labels\n",
    "z_true = np.array([0]*100 + [1]*100)\n",
    "\n",
    "# Loop over data-generating eta values\n",
    "for i, eta in enumerate(tqdm(eta_values, desc=\"Eta grid\")):\n",
    "    for j in range(M):\n",
    "        # Generate synthetic data\n",
    "        X, means = generate_embeddings(N=N, separation=cluster_sep, sigma=sigma, z_true=z_true)\n",
    "        W, word_probs = generate_words(N=N, V=V, doc_length=doc_length, eta=eta, z_true=z_true)\n",
    "        js_divergence[i, j] = jensenshannon(word_probs[0], word_probs[1]) # use this as proxy for eta on later plot\n",
    "\n",
    "        gmm = GaussianMixture(n_components=K, covariance_type='full', random_state=42)\n",
    "        gmm.fit(X)\n",
    "        initial_labels = gmm.predict(X)\n",
    "\n",
    "        # Set NIW priors\n",
    "        m_0 = np.zeros(X.shape[1])  # Mean vector for the prior\n",
    "        \n",
    "        v = np.zeros(d)\n",
    "        for k in range(K):\n",
    "            cluster_k = X[initial_labels == k]\n",
    "            if len(cluster_k) == 0:\n",
    "                raise ValueError(f\"Cluster {k} has no points assigned.\")\n",
    "            v += np.var(cluster_k, axis=0) / K\n",
    "\n",
    "        S_0 = np.diag(v)  # Shared across all clusters\n",
    "        m_0 = np.zeros(d)  # Mean vector for the prior\n",
    "        kappa_0 = 0.05\n",
    "\n",
    "        # Initialise model using GMM labels as predictions\n",
    "        model = ClusterModel(X, W, K, initial_labels, m_0, S_0, kappa_0=kappa_0)\n",
    "\n",
    "        n_iter = 100\n",
    "        burnin = 50\n",
    "\n",
    "        samples, _ = run_gibbs(model, n_iter, burnin)\n",
    "        z_pred = estimate_cluster(samples, K)\n",
    "\n",
    "        # Compute ARI between true and inferred clusters\n",
    "        ari_values[i, j] = adjusted_rand_score(z_true, z_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Calculate median ARI and standard deviation\n",
    "ari_medians = np.median(ari_values, axis=1)\n",
    "\n",
    "# Plotting\n",
    "ax1.plot(eta_values, ari_medians, 'o', color='orange')\n",
    "ax1.set_xlabel(r'$\\eta$')\n",
    "ax1.set_ylabel('Median ARI')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Calculate mean ARI and standard deviation\n",
    "ari_medians = np.mean(ari_values, axis=1)\n",
    "\n",
    "# Fit a linear regression to (eta_values, ari_medians)\n",
    "slope, intercept, r_value, p_value, std_err = linregress(eta_values, ari_medians)\n",
    "\n",
    "# Add line of best fit only below ARI = 1\n",
    "fit_line = slope * eta_values + intercept\n",
    "\n",
    "ax2.plot(eta_values, ari_medians, 'o', label='Mean ARI', color='orange')\n",
    "\n",
    "ax2.set_xlabel(r'$\\eta$')\n",
    "ax2.set_ylabel('Mean ARI')\n",
    "ax2.legend()\n",
    "\n",
    "plt.savefig(PLOTS_DIR + 'ari_vs_eta.png')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(ari_values.T, positions=eta_values, widths=0.3, patch_artist=True,\n",
    "            boxprops=dict(facecolor='orange', alpha=0.6), \n",
    "            medianprops=dict(color='black'), \n",
    "            flierprops=dict(marker='o', markerfacecolor='red', markersize=4, linestyle='none'))\n",
    "\n",
    "plt.xlabel(r'$\\eta$')\n",
    "plt.ylabel('ARI')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR + 'ari_boxplot_vs_eta.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between pairwise JS divergence of word distributions and ARI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten for plotting\n",
    "flat_js = js_divergence.flatten()\n",
    "flat_ari = ari_values.flatten()\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(flat_js, flat_ari, alpha=0.4, s=20, c='royalblue', edgecolors='none')\n",
    "\n",
    "# Optional smoothing or trendline if desired\n",
    "# import seaborn as sns\n",
    "# sns.regplot(x=flat_js, y=flat_ari, lowess=True, scatter=False, color='black', line_kws={'linewidth': 1.5})\n",
    "\n",
    "plt.xlabel(r'JS($\\phi_1 \\mid \\phi_2)$', fontsize=12)\n",
    "plt.ylabel('ARI', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR + 'ari_vs_js_scatter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random initialisation, even for larger JS divergences, the model can sometimes be unable to recover the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment settings\n",
    "N, d, K, V = 200, 2, 2, 100\n",
    "cluster_sep, doc_length = 0.5, 100\n",
    "sigma = 0.5\n",
    "\n",
    "M = 50 # Number of simulations per eta\n",
    "eta_values = np.arange(0.1, 10.5, 0.5)\n",
    "ari_values = np.zeros((len(eta_values), M))\n",
    "js_divergence = np.zeros((len(eta_values), M))\n",
    "# Sample cluster labels\n",
    "z_true = np.array([0]*100 + [1]*100)\n",
    "\n",
    "# Loop over data-generating eta values\n",
    "for i, eta in enumerate(eta_values):\n",
    "    for j in range(M):\n",
    "        # Generate synthetic data\n",
    "        Y, means = generate_embeddings(N=N, separation=cluster_sep, sigma=sigma, z_true=z_true)\n",
    "        W, word_probs = generate_words(N=N, V=V, doc_length=doc_length, eta=eta, z_true=z_true)\n",
    "        js_divergence[i, j] = jensenshannon(word_probs[0], word_probs[1]) # use this as proxy for eta on later plot\n",
    "\n",
    "        # Fixed NIW prior\n",
    "        m_0 = np.mean(Y, axis=0)\n",
    "        kappa_0 = 5\n",
    "        nu_0 = d + 2\n",
    "        S_0 = np.cov(Y, rowvar=False) * d\n",
    "\n",
    "        # Initialise k means for initial cluster assignments\n",
    "        kmeans = KNeighborsClassifier(n_neighbors=K)\n",
    "        kmeans.fit(Y, z_true)\n",
    "        labels_kmeans = kmeans.predict(Y)\n",
    "\n",
    "        # Fit your model (using the true eta used for data generation)\n",
    "        model = ClusterModel(Y, W, K, gamma=1.0, eta=eta, m_0=m_0, kappa_0=kappa_0, nu_0=nu_0, S_0=S_0, initial_z=labels_kmeans)\n",
    "        sampler = CollapsedGibbsSampler(model, z_true)\n",
    "        sampler.run(5000, 1e-4, 2000)\n",
    "\n",
    "        # Compute ARI between true and inferred clusters\n",
    "        ari_values[i, j] = adjusted_rand_score(z_true, model.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing effectiveness of model with imbalanced clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "N = 200\n",
    "K = 2\n",
    "d, V = 2, 100\n",
    "doc_length = 20\n",
    "separation = 1.0\n",
    "sigma = 0.5\n",
    "eta = 1.0\n",
    "gamma = 1.0\n",
    "M = 30  # simulations per setting\n",
    "\n",
    "# Cluster imbalance settings (as proportions for cluster 0)\n",
    "cluster_0_props = np.linspace(0.5, 0.95, 10)  # from 50:50 to 95:5\n",
    "ari_scores = np.zeros((len(cluster_0_props), M))\n",
    "\n",
    "# Loop over imbalance settings\n",
    "for i, p0 in enumerate(tqdm(cluster_0_props)):\n",
    "    n0 = int(N * p0)\n",
    "    n1 = N - n0\n",
    "    z_true = np.array([0]*n0 + [1]*n1)\n",
    "\n",
    "    for j in range(M):\n",
    "        # Shuffle to avoid ordering effects\n",
    "        np.random.shuffle(z_true)\n",
    "\n",
    "        # Generate data\n",
    "        Y, means = generate_embeddings(N=N, separation=separation, sigma=sigma, z_true=z_true)\n",
    "        W, word_probs = generate_words(N=N, V=V, doc_length=doc_length, eta=eta, z_true=z_true)\n",
    "\n",
    "        # Model priors\n",
    "        m_0 = np.mean(Y, axis=0)\n",
    "        kappa_0 = 5\n",
    "        nu_0 = d + 2\n",
    "        S_0 = np.cov(Y, rowvar=False) * d\n",
    "\n",
    "        # Initial clustering (optional: GMM or KMeans)\n",
    "        gmm = GaussianMixture(n_components=K, random_state=42)\n",
    "        gmm.fit(Y)\n",
    "        labels_init = gmm.predict(Y)\n",
    "\n",
    "       # Set NIW priors\n",
    "        m_0 = np.zeros(X.shape[1])  # Mean vector for the prior\n",
    "        \n",
    "        v = np.zeros(d)\n",
    "        for k in range(K):\n",
    "            cluster_k = X[labels_init == k]\n",
    "            if len(cluster_k) == 0:\n",
    "                raise ValueError(f\"Cluster {k} has no points assigned.\")\n",
    "            v += np.var(cluster_k, axis=0) / K\n",
    "\n",
    "        S_0 = np.diag(v)  # Shared across all clusters\n",
    "        m_0 = np.zeros(d)  # Mean vector for the prior\n",
    "        kappa_0 = 0.05\n",
    "\n",
    "        # Initialise model using GMM labels as predictions\n",
    "        model = ClusterModel(X, W, K, labels_init, m_0, S_0, kappa_0=kappa_0)\n",
    "\n",
    "        n_iter = 100\n",
    "        burnin = 50\n",
    "\n",
    "        samples, _ = run_gibbs(model, n_iter, burnin)\n",
    "        z_pred = estimate_cluster(samples, K)\n",
    "\n",
    "        # Evaluate ARI\n",
    "        ari_scores[i, j] = adjusted_rand_score(z_true, z_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "mean_ari = np.mean(ari_scores, axis=1)\n",
    "std_ari = np.std(ari_scores, axis=1)\n",
    "\n",
    "lower_bound = np.clip(mean_ari - std_ari, 0, 1)\n",
    "upper_bound = np.clip(mean_ari + std_ari, 0, 1)\n",
    "\n",
    "plt.plot(cluster_0_props, mean_ari, marker='o', label='Mean ARI')\n",
    "plt.fill_between(cluster_0_props, lower_bound, upper_bound, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Proportion of Nodes in Cluster 0')\n",
    "plt.ylabel('Adjusted Rand Index (ARI)')\n",
    "plt.ylim(0, 1.05)  # Slight buffer above 1 to prevent cutting off error bars visually\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR + 'ari_vs_cluster_imbalance.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
